{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The game Environment\n",
    "\n",
    "We are supposed to make an ML agent for Yellow banana game. The aim of the agent is to collect as many yellow bananas as possible, while ignoring blue ones.  The game is a single agent game with four possible actions:\n",
    "* Up\n",
    "* Down\n",
    "* Left\n",
    "* Right\n",
    "The state space of the game is given by an array of size 37.\n",
    "\n",
    "\n",
    "## My Approach\n",
    "\n",
    "I make use of the simple DQN algorithm.\n",
    "\n",
    "### Hyperparameters\n",
    "I decided the hyperparameters:\n",
    "\n",
    "* Replay Buffer size: 100000. It will be used to intially store the random game and then teach the network by choosing random samples from it.\n",
    "\n",
    "* Batch size: 64. The number of samples taken from replay buffer at a time for training.\n",
    "\n",
    "* Discount Factor: 0.99. It tells how much importance my network gives to the future rewards. A large value means my agent will choose the actions which maximize the future rewards sometime even sacrificing the current reward.\n",
    "\n",
    "* Learning rate: 0.0005 It is a very slow learning rate to ensure stability. It is used by the network to update the weights via back propagation.\n",
    "\n",
    "* Update every: 4. This tells after how many episodes do we update the target Q-network.\n",
    "\n",
    "### Agent\n",
    "\n",
    "The agent is implemented through class Agent.\n",
    "\n",
    "### Replay Buffer\n",
    "The replay buffer which stores the experience is declared in class ReplayBuffer.\n",
    "\n",
    "The code for both the classes has been exactly taken from the class excercises.\n",
    "\n",
    "### Q Network\n",
    "\n",
    "Next we build a neural network which will approximate the Value function. It is a simple three layered feed forward network. The input layer has 37 units, the two hidden layers have 64 units each and the output layer has 4 units, one for each action.\n",
    "\n",
    "And lastly we define a function dqn which implements the DQN algorithm:\n",
    "    \n",
    "    1. Fill the replay buffer\n",
    "    2. For each episode\n",
    "        a) Find an action using Epsilon greedy policy.\n",
    "        b) Perform the action on the environment.\n",
    "        c) Add the action-state in replay buffer\n",
    "        d) update the Qpred network, and for every fourth run update the Q-target network.\n",
    "        Repeat steps a-d till the game is over (done = true)\n",
    "        \n",
    "        \n",
    "### Plot of reward\n",
    "\n",
    "![reward.png](reward.png)\n",
    "\n",
    "Above you can see the plot of rewards for roughly 1000 episodes. we can see that the reward is slowly increasing. I considered the game over when reward crossed a score of 16.\n",
    "\n",
    "### Future Plans\n",
    "\n",
    "* The first thing I would like to do is use the learned model to play the game in Unity environment itself.\n",
    "* Next, what I have implemented is a simple DQN, it would be great to both Duelling DQN and Double DQN.\n",
    "* Right now my agent takes the state as an array of size 37, it would be interesting if instead of state, it takes as input the raw image. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
